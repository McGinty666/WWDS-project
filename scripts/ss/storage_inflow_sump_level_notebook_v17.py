# -*- coding: utf-8 -*-
"""Storage_inflow_sump_level_notebook v17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18e_P166FxI35W3DngStNkWW5AcV8TzYz

RAF locking  addresses

195050SUMP LEVEL	B65788
195050PUMP 1 RS	B65796
195050PUMP 2 RS	B65797
195050RISING MAIN FLOW	E11726
"""

# Define the file paths (raw URLs). Link to wherever they are saved on your C drive etc.

#for linking to WestBexington data
#sump_level_file = 'https://raw.githubusercontent.com/McGinty666/Rainfall-and-spill-analysis/main/West%20bexingfton%20sump%20level%20from%20Jan%202024.xlsx'
#rainfall_file = 'https://raw.githubusercontent.com/McGinty666/Rainfall-and-spill-analysis/main/west%20bexington%20rainfall%20from%201-10-2023.xlsx'

"""# import key libraries"""

# Commented out IPython magic to ensure Python compatibility.
#import the neccessary libraries

import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import numpy as np
import requests
import io
# %matplotlib inline

"""# Retrieving data from Github"""

# Replace with your actual GitHub access token
Access_token = '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'

# File paths for Wwest Bexington rising main
path_bex_rising_main = 'WASTE_E18529.csv'

#Paths somerford
path_somerford_sump_level_1 = 'WASTE_E18527_partition_1.xlsx'
path_somerford_sump_level_2 = 'WASTE_E18527_partition_2.xlsx'
path_somerford_sump_level_3 = 'WASTE_E18527_partition_3.xlsx'
path_somerford_sump_level_4 = 'WASTE_E18527_partition_4.xlsx'
path_somerford_rainfall = '14035_Somerford_rainfall_01_01_20.xlsx'

# GitHub repository details
username = 'McGinty666'
repo = 'Datafiles_WWDS'
token = Access_token

def fetch_file_from_git(username, repo, path, token, skiprows=None):
    # Construct the URL for the raw file
    url = f'https://raw.githubusercontent.com/{username}/{repo}/main/{path}'

    # Fetch the file using the access token
    headers = {'Authorization': f'token {token}'}
    response = requests.get(url, headers=headers)

    # Check if the request was successful
    if response.status_code == 200:
        if path.endswith('.csv'):
            # Load the CSV data into a pandas DataFrame
            data = pd.read_csv(io.StringIO(response.text))
            return data
        elif path.endswith('.xlsx'):
            # Load the Excel data into a pandas DataFrame
            data = pd.read_excel(io.BytesIO(response.content), skiprows=skiprows, engine='openpyxl')
            return data
    else:
        print(f'Failed to fetch file {path}: {response.status_code}')
        return None

# Fetch the file and store it in a DataFrame
#flow_meter_fetch = fetch_file_from_git(username, repo, path_bex_rising_main, token)

#Fetch Somerford SPS data
sump_level_1_fetch = fetch_file_from_git(username, repo, path_somerford_sump_level_1, token)
sump_level_2_fetch = fetch_file_from_git(username, repo, path_somerford_sump_level_2, token)
sump_level_3_fetch = fetch_file_from_git(username, repo, path_somerford_sump_level_3, token)
sump_level_4_fetch = fetch_file_from_git(username, repo, path_somerford_sump_level_4, token)

"""# Import and clean the rainfall


A new column is created with a row wise mean rainfall intensity across all columns.


Update the start_row accordingly (varies depending on the number of grid tiles in the query, check where your first for of actual date values is)

Update the radar rainfall start time (exactly as given in your .red file) the code then adds the new timestamps adding 5 minutes to the row above.

"""

# Define the starting row and start time for rainfall data
start_row = 10
rainfall_start_time = datetime.strptime("01/01/2020 00:00", "%d/%m/%Y %H:%M")

rainfall_fetch = fetch_file_from_git(username, repo, path_somerford_rainfall, token, skiprows=start_row-1)

"""Clean the rainfall data, add a mean column


Note the radar rainfall may have missing data, normally signposted with a 99 value but sometimes a larger number (like 2490?). It can be useful to print the maximum values and replace with a 0 if you are happy to ignore the missing rows
"""

df_rainfall = rainfall_fetch

max_values = df_rainfall.iloc[:, 1:].max()
print(f'maximum values: {max_values}')

#replace the 99 values for a 0
df_rainfall.replace(99, 0, inplace=True)

#calulate the mean across the columns (could instead experiment with different grids tiles)
df_rainfall['mean rainfall'] = df_rainfall.iloc[:, 1:].mean(axis=1)
df_rainfall.insert(1, 'timestamp', [rainfall_start_time + timedelta(minutes=5*i) for i in range(len(df_rainfall))])
#in future you could add a col to the row in dataframe to signpost missing data

df_rainfall.head(5)

# Reading the data into dataframes
df_sump_1 = sump_level_1_fetch.iloc[:, :3]
df_sump_2 = sump_level_2_fetch.iloc[:, :3]
df_sump_3 = sump_level_3_fetch.iloc[:, :3]
df_sump_4 = sump_level_4_fetch.iloc[:, :3]

# Adding columns
for df in [df_sump_1, df_sump_2, df_sump_3, df_sump_4]:
    df.columns= ["signal", "time", "level"]
    df["time"] = pd.to_datetime(df["time"])

# Combining the dataframes
df_sump = pd.concat([df_sump_1, df_sump_2, df_sump_3, df_sump_4])

# Sorting by time
df_sump = df_sump.sort_values(by='time')

# Resetting index
df_sump = df_sump.reset_index(drop=True)

'''

df_flow_meter = flow_meter_fetch
df_flow_meter.columns = ['signal', 'date_time', 'flow']
df_flow_meter.head(5)

'''

"""# Import flow meter data and create daily flow profiles"""

# Convert 'date_time' to datetime
df_flow_meter['date_time'] = pd.to_datetime(df_flow_meter['date_time'])

# Extract date and hour
df_flow_meter['date'] = df_flow_meter['date_time'].dt.date
df_flow_meter['hour'] = df_flow_meter['date_time'].dt.hour


start_date_fm = pd.to_datetime('2024-04-13').date()
end_date_fm = pd.to_datetime('2024-04-25').date() # ideally you pick dry days - you could use antecendent rainfall fucntion or visual inspection

# Filter for the specified date range
df_filtered_flow_meter = df_flow_meter[(df_flow_meter['date'] >= start_date_fm) & (df_flow_meter['date'] <= end_date_fm)]


# Calculate the averaged hourly flow for each unique date
df_avg_hourly_flow = df_filtered_flow_meter.groupby(['date', 'hour'])['flow'].mean().reset_index()

# Pivot the DataFrame to have hours as columns and dates as rows
df_pivot = df_avg_hourly_flow.pivot(index='hour', columns='date', values='flow')

# Plotting
plt.figure(figsize=(10, 6))
for date in df_pivot.columns:
    plt.plot(df_pivot.index, df_pivot[date], marker='o', label=str(date))

plt.xlabel('Hour')
plt.ylabel('Averaged Flow')
plt.title('Averaged Hourly Flow for Each Date')
plt.legend(title='Date')
plt.grid(True)
plt.show()



# Calculate the median hourly flow across all dates within the specified date range
df_mean_hourly_flow = df_filtered_flow_meter.groupby('hour')['flow'].mean().reset_index()

# Plotting median hourly flow across all dates
plt.figure(figsize=(10, 6))
plt.plot(df_mean_hourly_flow['hour'], df_mean_hourly_flow['flow'], marker='o', label='Median Flow')

plt.xlabel('Hour')
plt.ylabel('Mean Flow')
plt.title('Mean Hourly Flow Across All Dates selected')
plt.legend(title='Flow Type')
plt.grid(True)
plt.show()



"""# Plotting the raw data rainfall and sump levels

You should only need to modify the start and end time, the function won't need tweaking.
"""

from IPython.display import display, clear_output
import ipywidgets as widgets

def plot_rainfall_and_sump_level(start_time, end_time):
    # Convert start and end times to datetime
    start_time = pd.to_datetime(start_time)
    end_time = pd.to_datetime(end_time)

    # Filter the dataframes based on the specified time interval
    df_sump_filtered = df_sump[(df_sump["time"] >= start_time) & (df_sump["time"] <= end_time)]
    df_rainfall_filtered = df_rainfall[(df_rainfall["timestamp"] >= start_time) & (df_rainfall["timestamp"] <= end_time)]

    # Create a figure and axis objects
    fig, ax1 = plt.subplots(figsize=(12, 6))

    # Plot sump level on the first y-axis
    ax1.plot(df_sump_filtered["time"], df_sump_filtered["level"], color='green', label='Sump Level')
    ax1.set_xlabel('Time')
    ax1.set_ylabel('Sump Level', color='green')
    ax1.tick_params(axis='y', labelcolor='green')

    # Create a second y-axis for rainfall
    ax2 = ax1.twinx()
    ax2.plot(df_rainfall_filtered["timestamp"], df_rainfall_filtered["mean rainfall"], color='blue', label='Rainfall')
    ax2.set_ylabel('Rainfall intensity (mm/h)', color='blue')
    ax2.tick_params(axis='y', labelcolor='blue')

    # Add title and grid
    plt.title('Rainfall and Sump Level Over Time')
    fig.tight_layout()
    plt.grid(True)

    # Show the plot
    plt.show()

# Define the time period for analysis
start_time = "2024-01-01 00:00:00"
end_time = "2024-06-01 00:00:00"

#Ensure there is leading data ahead of the start time to avoid spurrious results

# Define the time period for plot (if you want to change from above)
#start_time_plot = start_time
#end_time_plot = end_time
start_time_plot = "2024-01-01 00:00:00"
end_time_plot = "2024-06-01 00:00:00"


# Create buttons for panning left and right
pan_left_button = widgets.Button(description="Pan Left")
pan_right_button = widgets.Button(description="Pan Right")

# Function to update the plot based on the new time interval
def update_plot(pan_direction):
    global start_time_plot, end_time_plot
    if pan_direction == 'left':
        start_time_plot = pd.to_datetime(start_time_plot) - pan_interval
        end_time_plot = pd.to_datetime(end_time_plot) - pan_interval
    elif pan_direction == 'right':
        start_time_plot = pd.to_datetime(start_time_plot) + pan_interval
        end_time_plot = pd.to_datetime(end_time_plot) + pan_interval
    # Clear the previous plot
    clear_output(wait=True)
    # Display buttons again after clearing output
    display(pan_left_button, pan_right_button)
    # Plot the updated graph
    plot_rainfall_and_sump_level(start_time_plot, end_time_plot)

# Assign functions to button clicks
pan_left_button.on_click(lambda x: update_plot('left'))
pan_right_button.on_click(lambda x: update_plot('right'))

# Display buttons and initial plot
display(pan_left_button, pan_right_button)

# Runs the above function given the timeframe to produce a time series plot
plot_rainfall_and_sump_level(start_time_plot, end_time_plot)

# Define the pan interval (e.g., 1 day)
pan_interval = pd.Timedelta(days=30)

"""Does it look like the rainfall has anything to do with the occurence of a spill from a quick visualization? if the answer is obviously no, maybe don't bother trying the next step of training an ML algorithm..

# Calculation for antecedent rainfall and EA spill counting

Here we calculate how much rainfall you had in total over the antecedent hours (variable you can define) before your sump level datapoint. How far back do you look? maybe test different values, which gives better prediction accuracy? what does it say about catchment?
"""

# Experiment with this value
antecedent_hours = 1

#Only for multivariate logistic regression
#antecedent_hours_2 = 0


#future work needs to allow different thresholds to be specified over different sub-periods

# Filter the DataFrame based on the specified time range
df_sump_filtered = df_sump[(df_sump["time"] >= start_time) & (df_sump["time"] <= end_time)].copy()

def calculate_antecedent_rainfall(df, X, hours):
    end_time = datetime.strptime(X, "%d/%m/%Y %H:%M")
    start_time = end_time - timedelta(hours=hours)
    mask = (df['timestamp'] >= start_time) & (df['timestamp'] <= end_time)
    antecedent_rainfall = df.loc[mask, 'mean rainfall'].sum() / 12
    # antecedent_rainfall = np.percentile(df.loc[mask, 'mean rainfall'], 80)
    # Calculate peak rainfall over the previous 0.5 hours
    peak_start_time = end_time - timedelta(hours=0.5)
    peak_mask = (df['timestamp'] >= peak_start_time) & (df['timestamp'] <= end_time)
    peak_rainfall = df.loc[peak_mask, 'mean rainfall'].max()

    return antecedent_rainfall

def calculate_peak_antecedent_rainfall(df, X, hours):
    end_time = datetime.strptime(X, "%d/%m/%Y %H:%M")
    start_time = end_time - timedelta(hours=hours)
    mask = (df['timestamp'] >= start_time) & (df['timestamp'] <= end_time)
    antecedent_rainfall = df.loc[mask, 'mean rainfall'].sum() / 12
    # antecedent_rainfall = np.percentile(df.loc[mask, 'mean rainfall'], 80)
    # Calculate peak rainfall over the previous 0.5 hours
    peak_start_time = end_time - timedelta(hours=0.5)
    peak_mask = (df['timestamp'] >= peak_start_time) & (df['timestamp'] <= end_time)
    peak_rainfall = df.loc[peak_mask, 'mean rainfall'].max()

    return  peak_rainfall

# Create a new DataFrame to store the results
results = []

# Extract unique date and hour values
unique_dates_hours = df_sump_filtered["time"].dt.floor('H').unique()

# Calculate antecedent rainfall for each hour of each day in the sump level DataFrame
for date_hour in unique_dates_hours:
    date_hour_str = date_hour.strftime("%d/%m/%Y %H:%M")
    antecedent_rainfall = calculate_antecedent_rainfall(df_rainfall, date_hour_str, antecedent_hours)
    #date_hour_2 = date_hour- timedelta(hours=antecedent_hours_2)*100
    #date_hour_str_2 = date_hour_2.strftime("%d/%m/%Y %H:%M")
    #antecedent_rainfall_2 = calculate_antecedent_rainfall(df_rainfall, date_hour_str_2, antecedent_hours_2)
    #results.append({"date_hour": date_hour, "antecedent_rainfall": antecedent_rainfall, "antecedent_rainfall_2": antecedent_rainfall_2})
    results.append({"date_hour": date_hour, "antecedent_rainfall": antecedent_rainfall})


# Convert the results list to a DataFrame
df_results = pd.DataFrame(results)

df_results.head(5)

"""Calculate DWF based on average rising main flows

"""

# Try to set this at the apparent spill level
threshold = 1600

# Add a boolean column to report if the threshold is exceeded

df_results["threshold_exceeded"] = df_results["date_hour"].apply(
    lambda x: any(df_sump_filtered[df_sump_filtered["time"].dt.floor('H') == x]["level"] > threshold)
)
def count_exceedance_instances(df):
    count = 0
    in_block = False
    block_start_time = None
# count the number of exceedances according to the EA 12/24 spill counting method
    for index, row in df.iterrows():
        if row["threshold_exceeded"]:
            if not in_block:
                # Start a new block
                in_block = True
                block_start_time = row["date_hour"]
                count += 1
            elif (row["date_hour"] - block_start_time) >= timedelta(hours=12):
                # Continue counting in 24-hour blocks
                if (row["date_hour"] - block_start_time) >= timedelta(hours=24):
                    count += 1
                    block_start_time = row["date_hour"]
        else:
            if in_block and (row["date_hour"] - block_start_time) >= timedelta(hours=24):
                # End the block if there is a 24-hour period with no exceedance
                in_block = False
    return count

# Count the exceedance instances
exceedance_count = count_exceedance_instances(df_results)
print(f"Number of exceedance instances: {exceedance_count}")

df_results.head(10)

"""# Calculation of pump-out volumes and inferred storage vs sump level using  flow meter readings"""

# Define spill_threshold and on_level_threshold
spill_threshold = threshold
on_level_threshold= 800
on_level_upper_limit = 850
sump_level_lower_limit = 1500

import pandas as pd

# Assuming df_sump_filtered is your original dataframe
df_copy = df_sump_filtered[['signal', 'time', 'level']].copy()

# Add a new boolean column 'Falling'
df_copy['Falling'] = df_copy['level'].le(df_copy['level'].shift())

# Subset to a new dataframe all periods where there is a constant fall
falling_periods = []
current_period = []

for index, row in df_copy.iterrows():
    if row['Falling'] and threshold >= row['level'] >= on_level_threshold:
        current_period.append(row)
    else:
        if current_period:  # End of falling period
            falling_periods.append(current_period)
            current_period = []

# Add the last period if it was falling
if current_period:
    falling_periods.append(current_period)

# Convert list of periods into separate dataframes
falling_dfs = [pd.DataFrame(period) for period in falling_periods if period]

# Filter out periods based on the first and last 'level' values
filtered_falling_dfs = []
for df in falling_dfs:
    if df['level'].iloc[0] >= sump_level_lower_limit and df['level'].iloc[-1] <= on_level_upper_limit:
        filtered_falling_dfs.append(df)

# Enumerate all individual falling periods
for i, df in enumerate(filtered_falling_dfs):
    df['falling_period_id'] = i

# Combine all individual dataframes back into one dataframe if needed
final_df_falling_periods = pd.concat(filtered_falling_dfs).reset_index(drop=True)

# Assuming final_df_falling_periods is your dataframe from the previous steps
# Reset the time to 0 for each falling_period_id
final_df_falling_periods['time_reset'] = final_df_falling_periods.groupby('falling_period_id')['time'].transform(lambda x: x - x.min())
final_df_falling_periods['time_reset_seconds'] = final_df_falling_periods['time_reset'].dt.total_seconds()

# Plotting
plt.figure(figsize=(10, 6))

for period_id, group in final_df_falling_periods.groupby('falling_period_id'):
    plt.plot(group['time_reset_seconds'], group['level'], label=f'Period {period_id}')

plt.xlabel('time_reset_seconds')
plt.ylabel('Level')
plt.title('Falling Periods Overlay - Spill level to ON level')
plt.legend()
plt.show()

"""The above needs to be subset using common sense - avoid including periods where there is clear rainfall runnoff"""

import pandas as pd
import numpy as np

# Assuming final_df_falling_periods and flow_meter_file are already loaded as DataFrames

# Ensure the 'time' columns are in datetime format
print( df_flow_meter.iloc[:, 1])

"""This adds a new column with the median value of flows(from rising main date) between the current and previous value in the sump analog"""

import pandas as pd


df_flow_meter.iloc[:, 1] = pd.to_datetime(df_flow_meter.iloc[:, 1])
# Function to calculate median value using iloc
def calculate_median(row, df_flow_meter):
    current_time = row['time']
    previous_time = final_df_falling_periods.iloc[row.name - 1]['time'] if row.name > 0 else current_time
    mask = (df_flow_meter.iloc[:, 1] >= previous_time) & (df_flow_meter.iloc[:, 1] <= current_time)
    median_value = df_flow_meter.loc[mask, df_flow_meter.columns[2]].median()
    return median_value

# Apply the function to each row
final_df_falling_periods['median_value'] = final_df_falling_periods.apply(calculate_median, axis=1, df_flow_meter=df_flow_meter)

final_df_falling_periods.head(5)

#final_df_falling_periods = pd.DataFrame(data_final)
#final_df_falling_periods['time'] = pd.to_datetime(final_df_falling_periods['time'])

# Function to calculate pumped out volume
def calculate_pumped_out_volume(row):
    if row.name == 0 or final_df_falling_periods.iloc[row.name]['falling_period_id'] != final_df_falling_periods.iloc[row.name - 1]['falling_period_id']:
        return 0
    else:
        seconds_increment = (final_df_falling_periods.iloc[row.name]['time_reset_seconds'] - final_df_falling_periods.iloc[row.name - 1]['time_reset_seconds'])
        return row['median_value'] * seconds_increment/ 1000   # Convert to cubic meters

# Apply the function to each row
final_df_falling_periods['pumped_out_volume'] = final_df_falling_periods.apply(calculate_pumped_out_volume, axis=1)

final_df_falling_periods.head(10)

final_df_falling_periods['cumulative_vol_pumped_out'] = final_df_falling_periods.groupby('falling_period_id')['pumped_out_volume'].cumsum()

# Plotting the graph
plt.figure(figsize=(10,6))

# Loop through each unique falling_period_id and plot the corresponding data
for period_id in final_df_falling_periods['falling_period_id'].unique():
    period_data = final_df_falling_periods[final_df_falling_periods['falling_period_id'] == period_id]
    plt.plot(period_data['time_reset_seconds'], period_data['cumulative_vol_pumped_out'], label=f'Period {period_id}')

# Adding labels and title
plt.xlabel('Time Reset Seconds')
plt.ylabel('Cumulative Volume Pumped Out')
plt.title('Cumulative Volume Pumped Out vs Time Reset Seconds')
plt.legend()

# Show the plot
plt.show()

"""**Requires DWF / additional rainfall adjustment**"""

max_vol = 32 #m3obtain from inspection from the above graph

# Create the 'storage_below' column
final_df_falling_periods['storage_below'] = max_vol - final_df_falling_periods['cumulative_vol_pumped_out']

# Plotting the scatter graph
plt.figure(figsize=(10,6))

# Loop through each unique falling_period_id and plot the corresponding data with different colors
for period_id in final_df_falling_periods['falling_period_id'].unique():
    period_data = final_df_falling_periods[final_df_falling_periods['falling_period_id'] == period_id]
    plt.plot(period_data['storage_below'], period_data['level'], label=f'Period {period_id}')

# Adding labels and title
plt.xlabel('Storage Below')
plt.ylabel('Level')
plt.title('Storage Below vs Level')
plt.legend()

# Reverse the order of the x-axis
plt.gca().invert_xaxis()

# Show the plot
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Assuming final_df_falling_periods is your dataframe from the previous steps
# Reset the time to 0 for each falling_period_id
final_df_falling_periods['time_reset'] = final_df_falling_periods.groupby('falling_period_id')['time'].transform(lambda x: x - x.min())
final_df_falling_periods['time_reset_seconds'] = final_df_falling_periods['time_reset'].dt.total_seconds()

# Define the maximum volume
max_vol = 32  # m3, obtained from inspection from the above graph

# Create the 'storage_below' column
final_df_falling_periods['storage_below'] = max_vol - final_df_falling_periods['cumulative_vol_pumped_out']

# Define the list of periods to be subset
subset_periods = [0, 2, 3, 4, 6]  # Example list of periods

# Filter the dataframe for the subset of periods
df_subset = final_df_falling_periods[final_df_falling_periods['falling_period_id'].isin(subset_periods)]

# Sort the data by 'level' to avoid the line going back and forth
df_subset = df_subset.sort_values(by='level')

# Prepare the data for polynomial fitting
A = np.vstack([df_subset['level']**2, df_subset['level'], np.ones(len(df_subset['level']))]).T
y = df_subset['storage_below']

# Perform the polynomial least squares fit
coeffs, _, _, _ = np.linalg.lstsq(A, y, rcond=None)

# Evaluate the polynomial fit
poly_fit = coeffs[0] * df_subset['level']**2 + coeffs[1] * df_subset['level'] + coeffs[2]

# Plotting the scatter graph
plt.figure(figsize=(10, 6))

# Loop through each unique falling_period_id and plot the corresponding data with different colors
for period_id in df_subset['falling_period_id'].unique():
    period_data = df_subset[df_subset['falling_period_id'] == period_id]
    plt.plot(period_data['level'], period_data['storage_below'], label=f'Period {period_id}')

# Plot the polynomial fit for all data
plt.plot(df_subset['level'], poly_fit, linestyle='--', color='black', label='Poly Fit All Data')

# Adding labels and title
plt.xlabel('Level')
plt.ylabel('Storage Below')
plt.title('Level vs Storage Below')
plt.legend()

# Show the plot
plt.grid(True)
plt.show()

# Print the coefficients
print(f'Coefficients for the polynomial fit of all data: {coeffs}')

# Print the polynomial formula
print(f'The polynomial formula is: y = {coeffs[0]:.9f}x^2 + {coeffs[1]:.9f}x + {coeffs[2]:.9f}')

"""Plot individual spill events according to the 12/24 criteria

# Inferring inflows from sump rise rate from the ON to spill level

This filters out the periods where you have a rise from On to spill level. it allows a single period of dropping level so you have more data to work with, not "strictly" rising from On to spill level
"""

#df_copy_rising = df_copy
#relaxation factor

# Assuming df_sump_filtered is your original dataframe
df_copy_rising = df_sump_filtered[['signal', 'time', 'level']].copy()

# Add a new boolean column 'Rising'
df_copy_rising['Rising'] = df_copy_rising['level'].ge(df_copy_rising['level'].shift())

# Subset to a new dataframe all periods where there is a constant rise
rising_periods = []
current_period = []
consecutive_drops = 0  # Track consecutive drops

for index, row in df_copy_rising.iterrows():
    if row['Rising'] and threshold >= row['level'] >= on_level_threshold:
        current_period.append(row)
        consecutive_drops = 0  # Reset consecutive drops
    else:
        if not row['Rising']:
            consecutive_drops += 1
        if consecutive_drops <= 1:
            current_period.append(row)
        else:
            if current_period:  # End of rising period
                rising_periods.append(current_period)
                current_period = []
                consecutive_drops = 0  # Reset consecutive drops

# Add the last period if it was rising
if current_period:
    rising_periods.append(current_period)

# Convert list of periods into separate dataframes
rising_dfs = [pd.DataFrame(period) for period in rising_periods if period]

print(f'rising_dfs: {len(rising_dfs)}')  # Print the number of rising dataframes

# Filter out periods based on the first and last 'level' values
filtered_rising_dfs = []
for df in rising_dfs:
    if df['level'].iloc[0] <= on_level_upper_limit and df['level'].iloc[-1] >= sump_level_lower_limit:
      filtered_rising_dfs.append(df)

# Enumerate all individual rising periods
for i, df in enumerate(filtered_rising_dfs):
    df['rising_period_id'] = i

# Combine all individual dataframes back into one dataframe if needed
final_df_rising_periods = pd.concat(filtered_rising_dfs).reset_index(drop=True)

# Assuming final_df_rising_periods is your dataframe from the previous steps
# Reset the time to 0 for each rising_period_id
final_df_rising_periods['time_reset'] = final_df_rising_periods.groupby('rising_period_id')['time'].transform(lambda x: x - x.min())
final_df_rising_periods['time_reset_seconds'] = final_df_rising_periods['time_reset'].dt.total_seconds()

# Filter out rising periods with fewer than 4 time series data points
filtered_rising_periods = final_df_rising_periods.groupby('rising_period_id').filter(lambda x: len(x) >= 4)

# Plotting
plt.figure(figsize=(10, 6))

for period_id, group in filtered_rising_periods.groupby('rising_period_id'):
    plt.plot(group['time_reset_seconds'], group['level'], label=f'Period {period_id}')

plt.xlabel('time_reset_seconds')
plt.ylabel('Level')
plt.title('Rising Periods Overlay - On to spill level')
plt.legend()
plt.show()

"""For the next stage we use this formula


$\frac{dS}{dt} = Q_{\text{in (rainfall-induced)}} + Q_{\text{in (DWF)}} - Q_{\text{out (flow meter)}}$


Which is rearranged to obtain the rainfall response

$Q_{\text{in (rainfall-induced)}} = \frac{dS}{dt} - Q_{\text{in (DWF)}} + Q_{\text{out (flow meter)}}$

Using the dS/dt is obained by mutliplying the rises in analog level by the dV/dlevel obtained in the falling analysis

Print the coefficients

y = 0.000020426x^2 + -0.006323343x + -7.696721867
"""

# Print the polynomial formula
print(f'The polynomial formula is: y = {coeffs[0]:.9f}x^2 + {coeffs[1]:.9f}x + {coeffs[2]:.9f}')

'''
a = float(input("Enter coefficient a: "))
b = float(input("Enter coefficient b: "))
c = float(input("Enter coefficient c: "))
'''
a= 0.000020426
b= -0.006323343
c= -7.696721867

filtered_rising_periods['storage below'] = a * filtered_rising_periods['level']**2 + b * filtered_rising_periods['level'] + c


def calculate_increased_volume(row):
    if row.name == 0 or filtered_rising_periods.at[row.name - 1, 'rising_period_id'] != row['rising_period_id']:
        return np.nan
    else:
        return row['storage below'] - filtered_rising_periods.at[row.name - 1, 'storage below']

# Apply the function to each row
filtered_rising_periods['increased_volume'] = filtered_rising_periods.apply(calculate_increased_volume, axis=1)

# Function to calculate deltatime
def calculate_deltatime(row):
    if row.name == 0 or filtered_rising_periods.at[row.name - 1, 'rising_period_id'] != row['rising_period_id']:
        return np.nan
    else:
        return row['time_reset_seconds'] - filtered_rising_periods.at[row.name - 1, 'time_reset_seconds']

# Apply the function to each row
filtered_rising_periods['deltatime'] = filtered_rising_periods.apply(calculate_deltatime, axis=1)

# Calculate overall_inflow
filtered_rising_periods['overall_inflow'] = filtered_rising_periods.apply(
    lambda row: row['increased_volume'] / row['deltatime'] if not np.isnan(row['increased_volume']) else np.nan, axis=1
)

# Display the first 5 rows
filtered_rising_periods.head(5)


def calculate_median_rising(row, df_flow_meter):
    current_time = row['time']
    previous_time = filtered_rising_periods.iloc[row.name - 1]['time'] if row.name > 0 else current_time
    mask = (df_flow_meter.iloc[:, 1] >= previous_time) & (df_flow_meter.iloc[:, 1] <= current_time)
    median_value = df_flow_meter.loc[mask, df_flow_meter.columns[2]].median()
    return median_value

filtered_rising_periods['median_value_Qout'] = filtered_rising_periods.apply(calculate_median_rising, axis=1, df_flow_meter=df_flow_meter)

filtered_rising_periods['time'] = pd.to_datetime(filtered_rising_periods['time'])

# Extract hour from 'time' column
filtered_rising_periods['hour'] = filtered_rising_periods['time'].dt.hour

# Map the 'flow' value from df_mean_hourly_flow to the corresponding 'hour' in filtered_rising_periods
filtered_rising_periods = filtered_rising_periods.merge(df_mean_hourly_flow, on='hour', how='left')

# Rename the 'flow' column to 'DWF_allowance'
filtered_rising_periods.rename(columns={'flow': 'DWF_allowance'}, inplace=True)

#print(filtered_rising_periods)

""" 'storm_response_inflow' calculated row by row as the  'overall_inflow '+ 'median_value_Qout' - 'DWF_allowance'"""

# Calculate 'storm_response_inflow'
filtered_rising_periods['storm_response_inflow'] = (
    filtered_rising_periods['overall_inflow'] +
    filtered_rising_periods['median_value_Qout'] -
    filtered_rising_periods['DWF_allowance']
)

filtered_rising_periods.head()

# Loop through each unique rising_period_id and plot the corresponding data
for period_id in filtered_rising_periods['rising_period_id'].unique():
    subset = filtered_rising_periods[filtered_rising_periods['rising_period_id'] == period_id]
    plt.plot(subset['storm_response_inflow'], subset['level'], label=f'Period {period_id}')

plt.xlabel('Storm Response Inflow')
plt.ylabel('Level')
plt.title('Storm Response Inflow vs Level for Different Rising Periods')
plt.legend()
plt.grid(True)
plt.show()

# Assuming filtered_rising_periods is your dataframe and it has a column 'date_hour'
filtered_rising_periods['antecedent_rainfall_peak'] = filtered_rising_periods['time'].apply(
    lambda date_hour: calculate_peak_antecedent_rainfall(df_rainfall, date_hour.strftime("%d/%m/%Y %H:%M"), antecedent_hours)
)

# Assuming filtered_rising_periods is your dataframe and it has columns 'period_id', 'antecedent_rainfall_peak', and 'storm_responsive_inflow'

# Group the dataframe by 'period_id'
grouped = filtered_rising_periods.groupby('rising_period_id')

# Create a figure
plt.figure()

# Plot for each unique period_id with different colors
for period_id, group in grouped:
    plt.scatter(group['antecedent_rainfall_peak'], group['storm_response_inflow'], label=f'Period ID: {period_id}')

# Add title and labels
plt.title('Storm Response Inflow vs Antecedent Rainfall Peak')
plt.xlabel('Antecedent Rainfall Peak')
plt.ylabel('Storm Response Inflow')
plt.grid(True)
plt.legend()
plt.show()

"""# Interesting exceedance statistics

How long do spills last?
How much rainfall is there leading the spills?


(could also test how long spill go on for following rainfall ceasing)
"""

# Calculate the total antecedent rainfall before each spill block starts
antecedent_rainfalls = []
exceedance_durations = []
block_start_times = []
in_block = False
block_start_time = None
total_antecedent_rainfall = 0

for index, row in df_results.iterrows():
    if row["threshold_exceeded"]:
        if not in_block:
            # Start a new block
            in_block = True
            block_start_time = row["date_hour"]
            block_start_times.append(block_start_time)
            total_antecedent_rainfall = row["antecedent_rainfall"]
        else:
            # Continue the block
            total_antecedent_rainfall += row["antecedent_rainfall"]
    else:
        if in_block:
            # End the block
            in_block = False
            antecedent_rainfalls.append(total_antecedent_rainfall)
            exceedance_duration = (row["date_hour"] - block_start_time).total_seconds() / 3600
            exceedance_durations.append(exceedance_duration)
            total_antecedent_rainfall = 0

print (block_start_times)

df_blocks = pd.DataFrame({
    'block_start_time': block_start_times,
    'start_time': [time - timedelta(hours=2) for time in block_start_times],
    'end_time': [time + timedelta(hours=48) for time in block_start_times] #could change this to add exceedance duration
})

print(df_blocks.head(5))

# Subset the desired spill blocks to inspect
subset_df_blocks = df_blocks.iloc[10:12]


# Iterate over the subset and run the function
for index, row in subset_df_blocks.iterrows():
    plot_rainfall_and_sump_level(row['start_time'], row['end_time'])

# Plot histogram of total antecedent rainfall
plt.figure(figsize=(6, 3))
plt.hist(antecedent_rainfalls, bins=10, color='blue', edgecolor='black')
plt.xlabel('Total Antecedent Rainfall (mm)')
plt.ylabel('Frequency')
plt.title(f'Total Antecedent Rainfall (over prior {antecedent_hours} hours) before Spill Block Starts from {start_time} to {end_time}')
plt.grid(True)
plt.show()

# Plot histogram of exceedance durations
plt.figure(figsize=(6, 3))
plt.hist(exceedance_durations, bins=10, color='green', edgecolor='black')
plt.xlabel('Exceedance Duration (hours)')
plt.ylabel('Frequency')
plt.title(f'Histogram of Exceedance Durations from {start_time} to {end_time} ')
plt.grid(True)
plt.show()

print('The median exceedance duration for this period is: '+ str(np.median(exceedance_durations))+' hours')

import pandas as pd
import seaborn as sns

df = df_results


# Create box plots with 1 row and 2 columns
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Box plot for antecedent_rainfall when threshold_exceeded is True
sns.boxplot(x='threshold_exceeded', y='antecedent_rainfall', data=df[df['threshold_exceeded'] == True], ax=axes[0])
axes[0].set_title('Antecedent Rainfall (Threshold Exceeded = True)')

# Box plot for antecedent_rainfall when threshold_exceeded is False
sns.boxplot(x='threshold_exceeded', y='antecedent_rainfall', data=df[df['threshold_exceeded'] == False], ax=axes[1])
axes[1].set_title('Antecedent Rainfall (Threshold Exceeded = False)')


plt.tight_layout()
plt.show()

"""# Getting into machine learning - predicting spills from rainfall

Preparing data into testing and training data ahead of the ML (It's good practice to split time series data chronologically)
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


df_results_filtered = df_results.sort_values(by="date_hour")
#df_results_filtered = df_results.drop(columns=['antecedent_rainfall_2']).copy()

# Define features (X) and target (y)
X = df_results_filtered[["antecedent_rainfall"]]
#X = df_results[["antecedent_rainfall", "antecedent_rainfall_2"]]  # Add other relevant features if needed
y = df_results_filtered["threshold_exceeded"]

# Split the data chronologically
split_index = int(len(df_results) * 0.75)
X_train, X_test = X[:split_index], X[split_index:]
y_train, y_test = y[:split_index], y[split_index:]

# If you were going to use this on NOT time series data, Split the data into training and testing sets (75/25 split, randomly)
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=24)

"""Running the actual ML algorith, reporting on accuracy


"""

# Train the logistic regression model
model = LogisticRegression() #There are settings you can modify
model.fit(X_train, y_train)

# Print the model parameters
print("Model Coefficients:", model.coef_)
print("Model Intercept:", model.intercept_)

# Predict on the test set and calculate accuracy
y_pred_test = model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred_test)
print(f"Test Accuracy: {test_accuracy}")

# Predict on the training set and calculate accuracy
y_pred_train = model.predict(X_train)
train_accuracy = accuracy_score(y_train, y_pred_train)
print(f"Training Accuracy: {train_accuracy}")

# Plot the S curve with correctly and incorrectly classified points in different colours
plt.figure(figsize=(10, 6))

# Correctly classified points
correctly_classified = (y_test == y_pred_test)
plt.scatter(X_test[correctly_classified], y_test[correctly_classified], color='black', label='Correctly Classified')

# Misclassified points
misclassified = (y_test != y_pred_test)
plt.scatter(X_test[misclassified], y_test[misclassified], color='red', label='Misclassified')

# Plot the logistic regression curve
X_range = np.linspace(X_test.min(), X_test.max(), 300).reshape(-1, 1)
y_prob = model.predict_proba(X_range)[:, 1]
plt.plot(X_range, y_prob, color='green', linewidth=2)

plt.xlabel("Antecedent Rainfall (mm)")
plt.ylabel("Probability of Threshold Exceeded")
plt.title("Logistic Regression S Curve, for antecedent rainfall over a " + str(antecedent_hours) + " hour period")
plt.legend()
plt.grid(True)
plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Train the logistic regression model
model = LogisticRegression() # There are settings you can modify
model.fit(X_train, y_train)

# Print the model parameters
print("Model Coefficients:", model.coef_)
print("Model Intercept:", model.intercept_)

# Predict on the test set and calculate accuracy
y_pred_test = model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred_test)
print(f"Test Accuracy: {test_accuracy}")

# Predict on the training set and calculate accuracy
y_pred_train = model.predict(X_train)
train_accuracy = accuracy_score(y_train, y_pred_train)
print(f"Training Accuracy: {train_accuracy}")

# Compute and display the confusion matrix
cm = confusion_matrix(y_test, y_pred_test)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.title("Confusion Matrix")
plt.show()

# Plot the S curve with correctly and incorrectly classified points in different colours
plt.figure(figsize=(10, 6))

# Correctly classified points
correctly_classified = (y_test == y_pred_test)
plt.scatter(X_test[correctly_classified], y_test[correctly_classified], color='black', label='Correctly Classified')

# Misclassified points
misclassified = (y_test != y_pred_test)
plt.scatter(X_test[misclassified], y_test[misclassified], color='red', label='Misclassified')

# Plot the logistic regression curve
X_range = np.linspace(X_test.min(), X_test.max(), 300).reshape(-1, 1)
y_prob = model.predict_proba(X_range)[:, 1]
plt.plot(X_range, y_prob, color='green', linewidth=2)

plt.xlabel("Antecedent Rainfall (mm)")
plt.ylabel("Probability of Threshold Exceeded")
plt.title("Logistic Regression S Curve, for antecedent rainfall over a " + str(antecedent_hours) + " hour period")
plt.legend()
plt.grid(True)
plt.show()

print(misclassified)

"""# Additional context on logistic regression and limitations

the equation for the logistic function is:


$$
P(y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1)}}
$$]

If it's multivariate

$$
P(y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n)}}
$$

Model coefficents are Beta_1, Beta_2 if multivariate, intercept is Beta_0
https://en.wikipedia.org/wiki/Logistic_regression#:~:text=Logistic%20regression%20is%20a%20supervised,based%20on%20patient%20test%20results.

Note the test/training accuracy reported accounts for many correctly classified points where there would have been no rainfall for days, therefore no spill predicted or reported, and even a badly fit S curve should get that right (maybe a modified accuracy for only accounting points with some rain over a threshold us called for?)


you can use logistic regression on time series data, but there are some important considerations to keep in mind.

Logistic regression is typically used for binary classification problems, where the goal is to predict the probability of a binary outcome (e.g., yes/no, true/false). When applying logistic regression to time series data, you need to account for the temporal dependencies in the data. Here are a few key points:




*   Feature Engineering: You may need to create lagged features, which are previous time steps of the variables, to capture the temporal dependencies. For example, if you’re predicting whether it will rain tomorrow, you might include features like whether it rained today and yesterday.

*   Stationarity: Time series data often exhibit trends and seasonality. It’s important to ensure that the data is stationary, meaning its statistical properties do not change over time. Techniques like differencing or detrending can help achieve stationarity.

*   Autocorrelation: Time series data often have autocorrelation, where past values are correlated with future values. This can violate the independence assumption of logistic regression. You might need to use techniques like autoregressive models or include autocorrelation terms in your logistic regression model.

*   Evaluation: When evaluating the performance of your model, use time series cross-validation methods rather than random cross-validation to respect the temporal order of the data.

2. Point-Biserial Correlation
The point-biserial correlation coefficient measures the strength and direction of the association between a continuous variable and a binary variable. It is a special case of the Pearson correlation coefficient.
"""